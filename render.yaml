services:
  - type: web
    name: civicos-backend
    env: node
    plan: starter
    buildCommand: |
      # Install system dependencies
      apt-get update && apt-get install -y curl wget
      
      # Install Ollama
      curl -fsSL https://ollama.ai/install.sh | sh
      
      # Build the application
      npm install
      npm run build
    startCommand: |
      # Start Ollama in background and wait for it to be ready
      echo "ü§ñ Starting Ollama service..."
      OLLAMA_HOST=0.0.0.0:11434 ollama serve &
      OLLAMA_PID=$!
      
      # Wait for Ollama to be ready
      echo "‚è≥ Waiting for Ollama to be ready..."
      sleep 15
      
      # Check if Mistral model is available
      echo "üîç Checking for Mistral model..."
      if ! ollama list | grep -q "mistral"; then
        echo "üì• Downloading Mistral model..."
        ollama pull mistral:latest
      else
        echo "‚úÖ Mistral model already available"
      fi
      
      # Test Ollama connection
      echo "üß™ Testing Ollama connection..."
      if curl -s http://127.0.0.1:11434/api/tags > /dev/null; then
        echo "‚úÖ Ollama is ready!"
      else
        echo "‚ùå Ollama connection failed"
      fi
      
      # Start the application
      echo "üöÄ Starting CivicOS backend..."
      npm start
    envVars:
      - key: NODE_ENV
        value: production
      - key: ENABLE_OLLAMA
        value: true
      - key: OLLAMA_HOST
        value: 0.0.0.0:11434
      - key: OLLAMA_BASE_URL
        value: http://127.0.0.1:11434
      - key: OLLAMA_MODEL
        value: mistral:latest
      - key: AI_SERVICE_ENABLED
        value: true
      - key: MISTRAL_ENABLED
        value: true 