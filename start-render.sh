#!/bin/bash

echo "ğŸš€ CIVICOS SUITE - RENDER STARTUP SCRIPT"
echo "========================================="

# Set production environment
export NODE_ENV=production
export RENDER=true

# Function to check if Ollama is running
check_ollama() {
    if curl -s --max-time 5 http://localhost:11434/api/tags > /dev/null 2>&1; then
        return 0
    else
        return 1
    fi
}

# Function to install Ollama
install_ollama() {
    echo "ğŸ¤– Installing Ollama..."
    
    # Download and install Ollama
    curl -fsSL https://ollama.ai/install.sh | sh
    
    # Add Ollama to PATH
    export PATH=$PATH:/usr/local/bin
    
    # Verify installation
    if command -v ollama &> /dev/null; then
        echo "âœ… Ollama installed successfully"
        return 0
    else
        echo "âŒ Ollama installation failed"
        return 1
    fi
}

# Function to start Ollama
start_ollama() {
    echo "ğŸ¤– Starting Ollama service..."
    
    # Kill any existing Ollama processes
    pkill -f ollama 2>/dev/null || true
    sleep 2
    
    # Start Ollama in background
    ollama serve > /dev/null 2>&1 &
    OLLAMA_PID=$!
    
    # Wait for Ollama to start
    echo "â³ Waiting for Ollama to be ready..."
    for i in {1..30}; do
        if check_ollama; then
            echo "âœ… Ollama is running successfully"
            return 0
        fi
        echo "â³ Attempt $i/30 - Waiting for Ollama..."
        sleep 2
    done
    
    echo "âŒ Ollama failed to start after 60 seconds"
    return 1
}

# Function to ensure Mistral model is available
ensure_mistral() {
    echo "ğŸ“¥ Ensuring Mistral model is available..."
    
    # Check if Mistral is already available
    if ollama list | grep -q "mistral"; then
        echo "âœ… Mistral model already available"
        return 0
    fi
    
    # Pull Mistral model
    echo "ğŸ“¥ Downloading Mistral model (this may take a few minutes)..."
    ollama pull mistral:latest
    
    if [ $? -eq 0 ]; then
        echo "âœ… Mistral model downloaded successfully"
        return 0
    else
        echo "âŒ Failed to download Mistral model"
        return 1
    fi
}

# Main startup sequence
echo "ğŸ”§ Starting CivicOS with AI service..."

# Step 1: Install Ollama if not available
if ! command -v ollama &> /dev/null; then
    if ! install_ollama; then
        echo "âš ï¸  Ollama installation failed, continuing without AI service"
    fi
fi

# Step 2: Start Ollama
if command -v ollama &> /dev/null; then
    if start_ollama; then
        # Step 3: Ensure Mistral is available
        if ensure_mistral; then
            echo "ğŸ‰ AI service is ready!"
            echo "ğŸ“‹ Available models:"
            ollama list
        else
            echo "âš ï¸  Mistral model not available, AI service will use fallbacks"
        fi
    else
        echo "âš ï¸  Ollama failed to start, AI service will use fallbacks"
    fi
else
    echo "âš ï¸  Ollama not available, AI service will use fallbacks"
fi

# Step 4: Start the main application
echo "ğŸš€ Starting CivicOS application on Render..."
echo "ğŸŒ Frontend: https://civicos.onrender.com"
echo "ğŸ”§ Backend: https://civicos.onrender.com/api"
echo "ğŸ¤– AI Service: https://civicos.onrender.com/api/ai"

# Start the Node.js application
exec NODE_ENV=production RENDER=true node dist/server/index.js 