#!/bin/bash

echo "ğŸš€ CIVICOS SUITE - RENDER STARTUP SCRIPT"
echo "========================================="

# Set production environment
export NODE_ENV=production
export RENDER=true

# Function to check if Ollama is running
check_ollama() {
    if curl -s --max-time 10 http://localhost:11434/api/tags > /dev/null 2>&1; then
        return 0
    else
        return 1
    fi
}

# Function to download and setup Ollama
download_ollama() {
    echo "ğŸ¤– Downloading Ollama..."
    
    # Create ollama directory
    mkdir -p ollama-bundle
    cd ollama-bundle
    
    # Download Ollama binary with retry logic
    echo "ğŸ“¥ Downloading Ollama binary..."
    for i in {1..3}; do
        if curl -L --max-time 300 https://github.com/ollama/ollama/releases/download/v0.9.6/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz; then
            echo "âœ… Ollama binary downloaded"
            
            # Verify download
            if [ -f "ollama-linux-amd64.tgz" ] && [ -s "ollama-linux-amd64.tgz" ]; then
                echo "ğŸ“¦ Download verified, extracting..."
                break
            else
                echo "âŒ Download verification failed"
                rm -f ollama-linux-amd64.tgz
                if [ $i -eq 3 ]; then
                    cd ..
                    return 1
                fi
                sleep 10
                continue
            fi
        else
            echo "âŒ Download attempt $i failed"
            if [ $i -eq 3 ]; then
                cd ..
                return 1
            fi
            sleep 10
        fi
    done
    
    # Extract the binary with better error handling
    echo "ğŸ“¦ Extracting Ollama binary..."
    if tar -xzf ollama-linux-amd64.tgz; then
        echo "ğŸ“‹ Contents after extraction:"
        ls -la
        
        # Find the actual ollama binary
        if [ -f "ollama" ]; then
            echo "âœ… Found ollama binary"
        elif [ -f "bin/ollama" ]; then
            echo "âœ… Found ollama in bin/ directory"
            mv bin/ollama ./ollama
        elif [ -f "*/ollama" ]; then
            echo "âœ… Found ollama in subdirectory"
            find . -name "ollama" -type f -exec mv {} ./ollama \;
        else
            echo "âŒ Ollama binary not found after extraction"
            echo "ğŸ“‹ Directory contents:"
            find . -type f -name "*ollama*"
            cd ..
            return 1
        fi
        
        # Make it executable
        chmod +x ollama
        
        # Verify it's executable
        if [ -x "ollama" ]; then
            echo "âœ… Ollama binary is executable"
        else
            echo "âŒ Failed to make ollama executable"
            cd ..
            return 1
        fi
        
        # Clean up
        rm -f ollama-linux-amd64.tgz
        
        echo "âœ… Ollama binary ready"
        cd ..
        return 0
    else
        echo "âŒ Failed to extract Ollama"
        cd ..
        return 1
    fi
}

# Function to start Ollama
start_ollama() {
    echo "ğŸ¤– Starting Ollama service..."
    
    # Kill any existing Ollama processes
    pkill -f ollama 2>/dev/null || true
    sleep 5
    
    # Start Ollama with better error handling
    cd ollama-bundle
    if [ -f "./ollama" ]; then
        nohup ./ollama serve > ollama.log 2>&1 &
        OLLAMA_PID=$!
        cd ..
        
        # Wait for Ollama to start with extended timeout
        echo "â³ Waiting for Ollama to be ready..."
        for i in {1..60}; do
            if check_ollama; then
                echo "âœ… Ollama is running successfully (PID: $OLLAMA_PID)"
                return 0
            fi
            echo "â³ Attempt $i/60 - Waiting for Ollama..."
            sleep 3
        done
        
        echo "âŒ Ollama failed to start after 3 minutes"
        cd ollama-bundle
        echo "Last 20 lines of Ollama log:"
        tail -n 20 ollama.log 2>/dev/null || echo "No log file found"
        cd ..
        return 1
    else
        echo "âŒ Ollama binary not found"
        cd ..
        return 1
    fi
}

# Function to ensure Mistral model is available
ensure_mistral() {
    echo "ğŸ“¥ Ensuring Mistral model is available..."
    
    if [ -f "./ollama-bundle/ollama" ]; then
        cd ollama-bundle
        
        # Check if Mistral is available
        if ./ollama list 2>/dev/null | grep -q "mistral"; then
            echo "âœ… Mistral model already available"
            cd ..
            return 0
        else
            echo "ğŸ“¥ Downloading Mistral model (this may take several minutes)..."
            
            # Set timeout for model download (20 minutes)
            timeout 1200 ./ollama pull mistral:latest
            if [ $? -eq 0 ]; then
                echo "âœ… Mistral model downloaded successfully"
                cd ..
                return 0
            else
                echo "âŒ Failed to download Mistral model or timed out"
                cd ..
                return 1
            fi
        fi
    else
        echo "âŒ Ollama not available for model download"
        return 1
    fi
}

# Main startup sequence
echo "ğŸ”§ Starting CivicOS with AI service..."

# Step 1: Check if Ollama exists, download if not
if [ ! -f "./ollama-bundle/ollama" ]; then
    echo "ğŸ“¥ Ollama not found, downloading..."
    if download_ollama; then
        echo "âœ… Ollama downloaded successfully"
    else
        echo "âŒ Failed to download Ollama - continuing without AI service"
    fi
else
    echo "âœ… Ollama already exists"
fi

# Step 2: Start Ollama if available (non-blocking)
if [ -f "./ollama-bundle/ollama" ]; then
    echo "ğŸš€ Attempting to start Ollama..."
    if start_ollama; then
        echo "ğŸ‰ Ollama started successfully!"
        
        # Step 3: Ensure Mistral is available (background process)
        (
            echo "ğŸ“¥ Checking Mistral model availability..."
            if ensure_mistral; then
                echo "ğŸ‰ AI service is fully ready!"
                echo "ğŸ“‹ Available models:"
                cd ollama-bundle
                ./ollama list
                cd ..
            else
                echo "âš ï¸  Mistral model not available, AI service will use fallbacks"
            fi
        ) &
    else
        echo "âš ï¸  Ollama failed to start, AI service will use fallbacks"
    fi
else
    echo "âš ï¸  Ollama not available, AI service will use fallbacks"
fi

# Step 4: Start the main application (always proceed)
echo "ğŸš€ Starting CivicOS application on Render..."
echo "ğŸŒ Frontend: https://civicos.onrender.com"
echo "ğŸ”§ Backend: https://civicos.onrender.com/api"
echo "ğŸ¤– AI Service: https://civicos.onrender.com/api/ai"

# Add environment variables for AI service
export AI_SERVICE_ENABLED=true
export OLLAMA_BASE_URL=http://localhost:11434
export OLLAMA_MODEL=mistral:latest

# Start the Node.js application (never fail)
export NODE_ENV=production
export RENDER=true
echo "ğŸš€ Executing: node dist/server/index.js"
exec node dist/server/index.js 